{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58ce26f-5cfd-4f08-998b-ab1682d22749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4102ac0f-b15c-434e-962a-237c9474c03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import deque\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5b9f238-0bab-412c-b379-2ecf17b218e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                                 | 0/50 [00:00<?, ?it/s]C:\\Users\\markov-consult\\AppData\\Local\\Temp\\ipykernel_4880\\3730523736.py:95: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  states = torch.FloatTensor(states).to(self.device)\n",
      "Training: 100%|█████████████████████████████████████████████████████████████████████| 50/50 [7:49:43<00:00, 563.67s/it]\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████| 10/10 [02:25<00:00, 14.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Results:\n",
      "Mean Reward: -143492.18\n",
      "Mean Utilization: 58.19%\n",
      "Mean Policy Error: 0.0087\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 3.4 Deep Q-Learning Architecture\n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # Shared feature layers\n",
    "        self.fc1 = nn.Linear(state_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        \n",
    "        # Dueling streams\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim))\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        value = self.value_stream(x)\n",
    "        advantage = self.advantage_stream(x)\n",
    "        \n",
    "        # Combine value and advantage streams\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "        return q_values\n",
    "\n",
    "# 3.5 Training Protocol\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=0.00025, gamma=0.99, \n",
    "                 epsilon=1.0, epsilon_min=0.05, epsilon_decay=0.995,\n",
    "                 batch_size=128, memory_size=10000, tau=0.01):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "        \n",
    "        # Device configuration\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Networks\n",
    "        self.policy_net = DuelingDQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_net = DuelingDQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        # Optimizer and loss\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.SmoothL1Loss()  # Huber loss\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        \n",
    "        # Training history\n",
    "        self.losses = []\n",
    "        self.rewards = []\n",
    "        self.utilization_gains = []\n",
    "        self.policy_errors = []\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        else:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.policy_net(state)\n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Sample batch from memory\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)\n",
    "        \n",
    "        # Compute Q-values\n",
    "        current_q = self.policy_net(states).gather(1, actions)\n",
    "        \n",
    "        # Compute target Q-values\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            target_q = rewards + (1 - dones) * self.gamma * next_q\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.loss_fn(current_q, target_q)\n",
    "        self.losses.append(loss.item())\n",
    "        \n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network\n",
    "        for target_param, policy_param in zip(self.target_net.parameters(), self.policy_net.parameters()):\n",
    "            target_param.data.copy_(self.tau * policy_param.data + (1.0 - self.tau) * target_param.data)\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save({\n",
    "            'policy_state_dict': self.policy_net.state_dict(),\n",
    "            'target_state_dict': self.target_net.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict()\n",
    "        }, path)\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.policy_net.load_state_dict(checkpoint['policy_state_dict'])\n",
    "        self.target_net.load_state_dict(checkpoint['target_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# 3.6 Environment and Reward Function\n",
    "class NetworkEnvironment:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.current_step = 0\n",
    "        self.n_steps = len(data)\n",
    "        \n",
    "        # Baseline weights from Delphi panel\n",
    "        self.base_weights = {\n",
    "            'Teaching': 0.28,\n",
    "            'Non-teaching': 0.12,\n",
    "            'Managerial': 0.35,\n",
    "            'Non-managerial': 0.25\n",
    "        }\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.current_weights = np.array([\n",
    "            self.base_weights['Teaching'],\n",
    "            self.base_weights['Non-teaching'],\n",
    "            self.base_weights['Managerial'],\n",
    "            self.base_weights['Non-managerial']\n",
    "        ])\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.current_weights = np.array([\n",
    "            self.base_weights['Teaching'],\n",
    "            self.base_weights['Non-teaching'],\n",
    "            self.base_weights['Managerial'],\n",
    "            self.base_weights['Non-managerial']\n",
    "        ])\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        row = self.data.iloc[self.current_step]\n",
    "        \n",
    "        # One-hot encode academic phase\n",
    "        phase = row['phase']\n",
    "        phase_encoding = {\n",
    "            'Lecture': [1, 0, 0],\n",
    "            'Examination': [0, 1, 0],\n",
    "            'Administrative': [0, 0, 1]\n",
    "        }\n",
    "        \n",
    "        # State vector: [volumes(4), capacity, weights(4), phase(3)]\n",
    "        state = np.array([\n",
    "            row['Teaching_volume'],\n",
    "            row['Non-teaching_volume'],\n",
    "            row['Managerial_volume'],\n",
    "            row['Non-managerial_volume'],\n",
    "            row['capacity'],\n",
    "            *self.current_weights,\n",
    "            *phase_encoding[phase]\n",
    "        ])\n",
    "        return state\n",
    "    \n",
    "    def _apply_action(self, action):\n",
    "        \"\"\"Apply action to adjust weights with constraints\"\"\"\n",
    "        # Action mapping: \n",
    "        # 0: +10% Teaching, 1: -10% Teaching\n",
    "        # 2: +10% Non-teaching, 3: -10% Non-teaching\n",
    "        # 4: +10% Managerial, 5: -10% Managerial\n",
    "        # 6: +10% Non-managerial, 7: -10% Non-managerial\n",
    "        \n",
    "        new_weights = self.current_weights.copy()\n",
    "        class_idx = action // 2\n",
    "        direction = 1 if action % 2 == 0 else -1\n",
    "        \n",
    "        # Calculate adjustment (10% of base weight)\n",
    "        adjustment = 0.1 * self.base_weights[list(self.base_weights.keys())[class_idx]] * direction\n",
    "        \n",
    "        # Apply adjustment\n",
    "        new_weights[class_idx] += adjustment\n",
    "        \n",
    "        # Maintain group constraints:\n",
    "        # Academic classes (Teaching + Non-teaching) = 0.4\n",
    "        # Non-academic classes (Managerial + Non-managerial) = 0.6\n",
    "        if class_idx in [0, 1]:  # Academic group\n",
    "            partner_idx = 1 if class_idx == 0 else 0\n",
    "            new_weights[partner_idx] = 0.4 - new_weights[class_idx]\n",
    "        else:  # Non-academic group\n",
    "            partner_idx = 3 if class_idx == 2 else 2\n",
    "            new_weights[partner_idx] = 0.6 - new_weights[class_idx]\n",
    "        \n",
    "        # Clip weights to ±20% of baseline\n",
    "        for i in range(4):\n",
    "            base = list(self.base_weights.values())[i]\n",
    "            min_val = max(0, base * 0.8)\n",
    "            max_val = min(1, base * 1.2)\n",
    "            new_weights[i] = np.clip(new_weights[i], min_val, max_val)\n",
    "        \n",
    "        return new_weights\n",
    "    \n",
    "    def _calculate_reward(self, state, new_weights):\n",
    "        \"\"\"Calculate reward based on institutional objectives\"\"\"\n",
    "        # Unpack state\n",
    "        (teach_vol, nt_vol, mgr_vol, nmgr_vol, \n",
    "         capacity, *_, phase1, phase2, phase3) = state\n",
    "        \n",
    "        # Calculate demand in Mbps (convert MB to Mbps)\n",
    "        interval_sec = 5 * 60  # 5-minute intervals\n",
    "        teach_demand = (teach_vol * 8) / interval_sec\n",
    "        nt_demand = (nt_vol * 8) / interval_sec\n",
    "        mgr_demand = (mgr_vol * 8) / interval_sec\n",
    "        nmgr_demand = (nmgr_vol * 8) / interval_sec\n",
    "        \n",
    "        # Calculate allocated bandwidth\n",
    "        teach_alloc = new_weights[0] * capacity\n",
    "        nt_alloc = new_weights[1] * capacity\n",
    "        mgr_alloc = new_weights[2] * capacity\n",
    "        nmgr_alloc = new_weights[3] * capacity\n",
    "        \n",
    "        # Calculate actual throughput\n",
    "        teach_tput = min(teach_alloc, teach_demand)\n",
    "        nt_tput = min(nt_alloc, nt_demand)\n",
    "        mgr_tput = min(mgr_alloc, mgr_demand)\n",
    "        nmgr_tput = min(nmgr_alloc, nmgr_demand)\n",
    "        \n",
    "        # Calculate satisfaction ratios\n",
    "        teach_sat = min(1, teach_tput / teach_demand) if teach_demand > 0 else 1\n",
    "        nt_sat = min(1, nt_tput / nt_demand) if nt_demand > 0 else 1\n",
    "        mgr_sat = min(1, mgr_tput / mgr_demand) if mgr_demand > 0 else 1\n",
    "        nmgr_sat = min(1, nmgr_tput / nmgr_demand) if nmgr_demand > 0 else 1\n",
    "        \n",
    "        # Get policy weights from state (positions 5-8 are weights)\n",
    "        teach_policy = state[5]\n",
    "        nt_policy = state[6]\n",
    "        mgr_policy = state[7]\n",
    "        nmgr_policy = state[8]\n",
    "        \n",
    "        # Calculate utilization\n",
    "        total_tput = teach_tput + nt_tput + mgr_tput + nmgr_tput\n",
    "        utilization = total_tput / capacity\n",
    "        \n",
    "        # Reward components\n",
    "        policy_compliance = (\n",
    "            teach_policy * teach_sat +\n",
    "            nt_policy * nt_sat +\n",
    "            mgr_policy * mgr_sat +\n",
    "            nmgr_policy * nmgr_sat\n",
    "        )\n",
    "        \n",
    "        underutilization_penalty = 0.15 * (capacity - total_tput)\n",
    "        \n",
    "        # Critical service assurance (Teaching during exams)\n",
    "        if state[10] == 1:  # Examination phase\n",
    "            teach_latency_penalty = 10 * max(0, teach_demand - teach_alloc) / teach_demand if teach_demand > 0 else 0\n",
    "        else:\n",
    "            teach_latency_penalty = 0\n",
    "        \n",
    "        # Final reward\n",
    "        reward = policy_compliance - underutilization_penalty - teach_latency_penalty\n",
    "        \n",
    "        # Track metrics\n",
    "        policy_error = np.mean(np.abs([\n",
    "            teach_policy - new_weights[0],\n",
    "            nt_policy - new_weights[1],\n",
    "            mgr_policy - new_weights[2],\n",
    "            nmgr_policy - new_weights[3]\n",
    "        ]))\n",
    "        \n",
    "        return reward, utilization, policy_error\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Apply action to get new weights\n",
    "        new_weights = self._apply_action(action)\n",
    "        \n",
    "        # Get current state\n",
    "        current_state = self._get_state()\n",
    "        \n",
    "        # Calculate reward based on current state and new weights\n",
    "        reward, utilization, policy_error = self._calculate_reward(current_state, new_weights)\n",
    "        \n",
    "        # Update weights for next state\n",
    "        self.current_weights = new_weights\n",
    "        \n",
    "        # Move to next time step\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.n_steps - 1\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = self._get_state() if not done else current_state\n",
    "        \n",
    "        return next_state, reward, done, utilization, policy_error\n",
    "\n",
    "# 3.7 Training Execution\n",
    "def train_dqn(env, agent, episodes=50, save_interval=5):\n",
    "    # Create directory for saving models and plots\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    os.makedirs(\"plots\", exist_ok=True)\n",
    "    \n",
    "    # Training loop\n",
    "    for episode in tqdm(range(episodes), desc=\"Training\"):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        episode_utilizations = []\n",
    "        episode_policy_errors = []\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, utilization, policy_error = env.step(action)\n",
    "            \n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            \n",
    "            total_reward += reward\n",
    "            episode_utilizations.append(utilization)\n",
    "            episode_policy_errors.append(policy_error)\n",
    "            \n",
    "            agent.replay()\n",
    "        \n",
    "        # Update metrics\n",
    "        agent.rewards.append(total_reward)\n",
    "        agent.utilization_gains.append(np.mean(episode_utilizations))\n",
    "        agent.policy_errors.append(np.mean(episode_policy_errors))\n",
    "        \n",
    "        # Decay epsilon\n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        # Save model periodically\n",
    "        if (episode + 1) % save_interval == 0:\n",
    "            agent.save_model(f\"models/dqn_epoch_{episode+1}.pth\")\n",
    "        \n",
    "        # Plot progress\n",
    "        if (episode + 1) % 5 == 0:\n",
    "            plot_training_progress(agent, episode+1)\n",
    "\n",
    "# 3.8 Visualization Functions\n",
    "def plot_training_progress(agent, episode):\n",
    "    plt.figure(figsize=(18, 12))\n",
    "    \n",
    "    # Loss\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(agent.losses)\n",
    "    plt.title(\"Training Loss (Huber)\")\n",
    "    plt.xlabel(\"Batch Updates\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Rewards\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(agent.rewards)\n",
    "    plt.title(\"Episode Total Reward\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Utilization\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(agent.utilization_gains)\n",
    "    plt.title(\"Bandwidth Utilization\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Utilization Rate\")\n",
    "    plt.ylim(0.5, 1.0)\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Policy Error\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(agent.policy_errors)\n",
    "    plt.title(\"Policy Weight Deviation\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"MAE\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"plots/training_progress_ep{episode}.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# 3.9 Validation and Testing\n",
    "def validate_model(env, agent, n_episodes=10):\n",
    "    test_rewards = []\n",
    "    test_utilizations = []\n",
    "    test_policy_errors = []\n",
    "    \n",
    "    for _ in tqdm(range(n_episodes), desc=\"Validation\"):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        episode_utilizations = []\n",
    "        episode_policy_errors = []\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, utilization, policy_error = env.step(action)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            episode_utilizations.append(utilization)\n",
    "            episode_policy_errors.append(policy_error)\n",
    "        \n",
    "        test_rewards.append(total_reward)\n",
    "        test_utilizations.append(np.mean(episode_utilizations))\n",
    "        test_policy_errors.append(np.mean(episode_policy_errors))\n",
    "    \n",
    "    # Plot validation results\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.bar(range(n_episodes), test_rewards)\n",
    "    plt.title(\"Validation Rewards\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.bar(range(n_episodes), test_utilizations)\n",
    "    plt.title(\"Bandwidth Utilization\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Utilization Rate\")\n",
    "    plt.ylim(0.6, 0.95)\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.bar(range(n_episodes), test_policy_errors)\n",
    "    plt.title(\"Policy Weight Deviation\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"MAE\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"plots/validation_results.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        'mean_reward': np.mean(test_rewards),\n",
    "        'mean_utilization': np.mean(test_utilizations),\n",
    "        'mean_policy_error': np.mean(test_policy_errors)\n",
    "    }\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load simulated data\n",
    "    data = pd.read_csv(\"network_traffic_simulation.csv\")\n",
    "    \n",
    "    # Initialize environment and agent\n",
    "    env = NetworkEnvironment(data)\n",
    "    state_dim = 12  # 4 volumes + 1 capacity + 4 weights + 3 phase indicators\n",
    "    action_dim = 8  # 8 discrete actions\n",
    "    agent = DQNAgent(state_dim, action_dim)\n",
    "    \n",
    "    # Train the model\n",
    "    train_dqn(env, agent, episodes=50)\n",
    "    \n",
    "    # Save final model\n",
    "    agent.save_model(\"models/dqn_final.pth\")\n",
    "    \n",
    "    # Validation\n",
    "    val_results = validate_model(env, agent)\n",
    "    print(\"\\nValidation Results:\")\n",
    "    print(f\"Mean Reward: {val_results['mean_reward']:.2f}\")\n",
    "    print(f\"Mean Utilization: {val_results['mean_utilization']:.2%}\")\n",
    "    print(f\"Mean Policy Error: {val_results['mean_policy_error']:.4f}\")\n",
    "    \n",
    "    # Plot final training progress\n",
    "    plot_training_progress(agent, \"final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b711b1-325a-49d4-83c7-b26afe5003cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
